---
title: "When do R programmers work?"
author: "Gilles Kratzer"
date: 2016-09-01T00:00:00
categories: ["R"]
tags: ["CRAN", "data visualization"]
---



<p><a id="IDdownload"></a></p>
<p>This post is highly inspired by <a href="https://www.r-bloggers.com/finally-tracking-cran-packages-downloads/">this post</a> and <a href="http://blog.kongscn.me/2015/05/15/r-packages-stats.html">this post</a>. <a href="#my_chunk">Here</a> is the necessary code to produce those graphs (be aware that it is time <strong>and</strong> memory consuming!).</p>
<p>On <a href="https://blog.rstudio.org/2013/06/10/rstudio-cran-mirror/">June 10 2013</a>, <a href="https://www.rstudio.com/">RStudio</a> provides CRAN mirror logs download from 2012-10-01. Then it is possible to analyze this <strong>rich and huge</strong> amount of data. On an individual level, one can track the popularity of their (preferred package). Here one can track the number of download of the three most downloaded R packages.</p>
<p>Here is the daily counts for the three most downloaded R packages, for May the 1 to September the 4th 2016. As one can see, there is an obvious weekly seasonality.</p>
<div style="width:800px; height=600px">
<center>
<img src="/img/images/dowloads_top3.png" />
</center>
</div>
<p>Here is the ordered total number of download per country for the same period. Rcpp seems to be the most used R package in all countries except Korea.</p>
<p> </p>
<div style="width:800px; height=600px">
<div class="figure">
<img src="/img/images/country_dowloads_top3.png" />

</div>
</div>
<p> </p>
<p>Here is the total count over the week days for this period. As one can see, number of downloads decreases of about a half during the weekend.</p>
<p> </p>
<div class="figure">
<img src="/img/images/downloads_weekdays_top3.png" />

</div>
<p> </p>
<p>Finally, this graph represents the instantly (smoothed by minute) number of downloads in standardized UTC. As one can see, less downloads during night and lunch time.</p>
<p> </p>
<div class="figure">
<img src="/img/images/downloads_hour_top3.png" />

</div>
<p> </p>
<p>Below is the necessary code to extract, manipulate and plot the data.</p>
<p>  <a id="my_chunk"></a></p>
<pre class="r"><code>###########################################################################
##Gilles Kratzer
##Analysis on CRAN Rstudio log files in order to compute to the number of downloads
##History :
## --2016/09/06 document created
## --2016/09/09 adding material
## --2016/09/21 plotting data
###########################################################################

###########################################################################
##Purpose: Analysis of the downloads of R packages
##Highly inspired by https://www.r-bloggers.com/finally-tracking-cran-packages-downloads/
##Highly inspired by http://blog.kongscn.me/2015/05/15/r-packages-stats.html
###########################################################################

##cleaning
rm(list = ls())
graphics.off()

##packages
library(lubridate)
library(data.table)
library(dplyr)
library(tidyr)
library(dplyr)
library(ggplot2)


## ======================================================================
## Step 1: Download all log files
## ======================================================================


# Here&#39;s an easy way to get all the URLs in R
# It downloads only the missing files
# !!! very long !!!

start &lt;- as.Date(&#39;2016-05-01&#39;)
today &lt;- as.Date(&#39;2016-10-15&#39;)

all_days &lt;- seq(start, today, by = &#39;day&#39;)

year &lt;- as.POSIXlt(all_days)$year + 1900
urls &lt;- paste0(&#39;http://cran-logs.rstudio.com/&#39;, year, &#39;/&#39;, all_days, &#39;.csv.gz&#39;)

# only download missing files
missing_days&lt;-setdiff(as.character(all_days), tools::file_path_sans_ext(dir(&quot;../3m/&quot;), TRUE))

dir.create(&quot;../3m/&quot;)
for (i in 1:length(missing_days)) {
  print(paste0(i, &quot;/&quot;, length(missing_days)))
  download.file(urls[i], paste0(&#39;../3m/&#39;, missing_days[i], &#39;.csv.gz&#39;))
}



## ======================================================================
## Step 2: Make a data table
## ======================================================================

#!!! time and memory consuming

DT = data.table()
files = list.files(&quot;../3m&quot;, pattern=&#39;*.gz&#39;, full.names=TRUE)

for (file in files){
  print(paste(&quot;Reading&quot;, file, &quot;...&quot;))
  dt = tbl_dt(read.csv(file, as.is=TRUE))
  dt = dt[, `:=`(datetime=ymd_hms(paste(date, time)),
                 r_version=as.factor(r_version),
                 r_arch=as.factor(r_arch),
                 r_os=as.factor(r_os),
                 package=as.factor(package),
                 date=NULL, time=NULL, ip_id=NULL, version=NULL)]
  DT = rbind(DT, dt)
}
rm(dt)
DT = tbl_dt(DT)
setkey(DT, package, country)
save(DT, file=&quot;../3m/3m_final.RData&quot;)
gc() # !!!

## ======================================================================
## Step 3: Analysis 
## ======================================================================

load(file = &quot;../3m/3m_final.RData&quot;)

by_package = DT[, .N, keyby=package][order(-N)]

# Top downloads
by_package

top3 = DT[.(c(&quot;Rcpp&quot;,&quot;plyr&quot;, &quot;ggplot2&quot;))]
top3[, `:=`(date=as.Date(datetime))]

top3_stat = top3[, .N, by=.(package, date)]
qplot(x=date, y=N, data=top3_stat, geom=&#39;line&#39;, color=package)

ggsave(filename = &quot;dowloads_top3.pdf&quot;)

by_package &lt;- DT[, .N, keyby=package][order(-N)]

top3[, `:=`(date=as.Date(datetime))]
pack_stat&lt;-top3[, .N, by=.(package, date)]
qplot(x=date, y=N, data=pack_stat, geom=&#39;line&#39;, color=package)

ggsave(filename = &quot;dowloads.pdf&quot;)

##country
pack_stat&lt;-top3[,.N,keyby=.(country, package)][order(-N)]
pack_stat_count_country&lt;-top3[,.N,keyby=.(country)][order(-N)]

pack_stat$country&lt;-factor(x = pack_stat$country,levels = pack_stat_count_country$country)
m = ggplot(pack_stat[1:40], aes(x=country, y=N, fill=package))
m + geom_bar(stat=&quot;identity&quot;) + coord_flip()
ggsave(filename = &quot;country_dowloads_top3.pdf&quot;)

##Download per day
top3$Weekdays &lt;- weekdays(top3$datetime)

pack_stat&lt;-top3[, .N, by=.(package, Weekdays)]
pack_stat$Weekdays&lt;-factor(pack_stat$Weekdays, levels = c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, 
                                                          &quot;Friday&quot;, &quot;Saturday&quot;,&quot;Sunday&quot;))
ggplot(aes(x=Weekdays, y=N, fill=package), data=pack_stat)+
  geom_bar(stat=&quot;identity&quot;)+
  theme_bw()+
  xlab(NULL)

ggsave(filename = &quot;downloads_weekdays_top3.pdf&quot;)

##downloads per minutes 
top3$specific_time &lt;- hour(top3$datetime)+ minute(top3$datetime)/60
  
pack_stat&lt;-top3[, .N, by=.(package, specific_time)]
qplot(x=specific_time, y=N, data=pack_stat, geom=&#39;line&#39;,color=package)+
  scale_x_continuous(breaks = c(0,3,6, 9, 12, 15,18,21,24), labels=c(
                                          &quot;00:00&quot;,
                                          &quot;03:00&quot;,
                                          &quot;06:00&quot;,
                                          &quot;09:00&quot;,
                                          &quot;12:00&quot;,
                                          &quot;15:00&quot;,
                                          &quot;18:00&quot;,
                                          &quot;21:00&quot;,
                                          &quot;24:00&quot;))+
  xlab(NULL)
  
ggsave(filename = &quot;downloads_hour_top3.pdf&quot;)</code></pre>
