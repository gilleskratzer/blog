<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>So what ... on So what ...</title>
    <link>/</link>
    <description>Recent content in So what ... on So what ...</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Gilles Kratzer</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Comparison between Suitable Priors for Additive Bayesian Networks</title>
      <link>/publication/catabn/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/catabn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Information-Theoretic Scoring Rules to Learn Additive Bayesian Network Applied to Epidemiology</title>
      <link>/publication/mleabn/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/mleabn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pododermatitis in group housed rabbit does in Switzerland – prevalence, severity and risk factors</title>
      <link>/publication/ruchti1/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/ruchti1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Progression and risk factors of pododermatitis in part-time group housed rabbit does in Switzerland</title>
      <link>/publication/ruchti2/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/ruchti2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Revealing the structure of the associations between housing system, facilities, management and welfare of commercial laying hens using Additive Bayesian Networks</title>
      <link>/publication/comin/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/comin/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A battle to the death</title>
      <link>/post/pval/</link>
      <pubDate>Thu, 03 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/pval/</guid>
      <description>&lt;p&gt;&lt;em&gt;It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel.&lt;/em&gt; Savage, The Foundations of Statistics (1972)&lt;/p&gt;
&lt;p&gt;Recentely, I had to give a presentation about &lt;strong&gt;P value&lt;/strong&gt; and I discovered during the preparation of this talk a &lt;strong&gt;battle to the death&lt;/strong&gt; within statistics. It is enough unique to be reported and commented.&lt;/p&gt;
&lt;p&gt;A little bit of &lt;a href=&#34;https://en.wikipedia.org/wiki/History_of_statistics&#34;&gt;history of statistics&lt;/a&gt;, between the 18th to the 19th century, Bernoulli, Bayes, Laplace, Boole, Venn and others developed lots of ideas about probability and statistics. At this time &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_inference&#34;&gt;statistical inference&lt;/a&gt;&lt;/strong&gt; was &lt;em&gt;Bayesian&lt;/em&gt;. Then it required prior probability. &lt;a href=&#34;https://en.wikipedia.org/wiki/Ronald_Fisher&#34;&gt;Sir Ronald Fisher&lt;/a&gt; didn’t want to specify a prior, so he constructed a new framework for statistical inference around 1920s. Part of this is what he called significance testing and significance levels, which we now call P-values. Jerzy Neyman and Egon Pearson “extended” Fisher’s ideas and wrote about hypothesis testing (1930s).&lt;/p&gt;
&lt;p&gt;Fisher wanted a way to test the results from experiments. The motivation questions he had were: How does the data match to my hypothesis (i.e the assumed model)? But he still refused to assume a prior probability. On the contrary Neyman does not assume many repeated experiments from the same population. He wanted the researcher to have a tool to help evaluate the strength of evidence.&lt;/p&gt;
&lt;p&gt;Fisher and Neyman couldn’t resolve the differences between their ideas. They argued for 25 years until Fisher died in 1962. Fisher proposed a way to measure how likely the results of an experiment are under some assumptions. Neyman proposed to use cut off levels to decide whether the data matched to a hypothesis, with the cut offs chosen to limit errors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>varrank: an R package for variable ranking based on mutual information with applications to observed systemic datasets</title>
      <link>/publication/varrank/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/varrank/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reproducibility in Science</title>
      <link>/post/natruecommunication/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/natruecommunication/</guid>
      <description>&lt;p&gt;As a biostatistician I am particularly concerned by reproducibility in research (&lt;strong&gt;RR&lt;/strong&gt;). I try very hard to do reproducible research. It is often hard. Often this is not clear how to achieve &lt;strong&gt;RR&lt;/strong&gt;. Within our group, we had recently some discussions about &lt;strong&gt;RR&lt;/strong&gt;. Below is a small &lt;em&gt;personal&lt;/em&gt; manifesto for &lt;strong&gt;RR&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;manifesto-for-reproducible-research&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Manifesto for reproducible research&lt;/h1&gt;
&lt;p&gt;I believe that good science is made of trustable science. I believe that the most trustable academic research can only be achieved with a rigorous application of scientific methods. I believe that a minimal condition of the scientific methods is reproducibility of the research. While being viewed as obvious consent within Academia, in practice it requires extremely well organised scientists. Statistics is by nature an interdisciplinary effort and as many other disciplines it faces the reproducibility crisis. If one want to produce relevant, widely accessible and trustable scientific outputs one has to take it very seriously.&lt;/p&gt;
&lt;p&gt;I view the reproducible research approach as a comprehensive philosophy. It includes the individual research of academic group members but also external collaborations. An essential part of reproducibility is the transparency of data. Therefore I tried to use publicly available and trustable data wherever possible and feasible. Likewise, I make my data product openly accessible together with the necessary documentation.&lt;/p&gt;
&lt;p&gt;I believe that scripting is the optimal way to achieve reproducible workflow. In order to create easily reproducible software packages, I follow the dynamic programming approach, which is a method to solve large scale problems by atomising them into simple tasks. Using version control allows me to document changes, ensuring historical reproducibility and efficient collaboration. I pay a special attention to publish the necessary documentation together with my softwares.&lt;/p&gt;
&lt;p&gt;Producing and delivering reproducible code implies being as independent as possible of the user environment. This is why I use platform independent and open source programming languages. It also requires to produce well documented code that corresponds to commonly used code styles, which facilitates user readability. I believe that the tests used to develop code are part of the code and then should be published.&lt;/p&gt;
&lt;p&gt;I believe that complex scientific challenges require large collaborative work to be tackled. I believe that reproducibility is of higher importance there. This is why I aim at working in an organised manner which means to be sparse with the documents I exchange to ensure efficient partnership. I believe that continuous integration is a way to save precious time.&lt;/p&gt;
&lt;p&gt;Reproducible research is a fast moving research area and I invest time for scooting new approaches and exchange with other research groups.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reproducibility in Science</title>
      <link>/post/repro/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/repro/</guid>
      <description>&lt;p&gt;As a biostatistician I am particularly concerned by reproducibility in research (&lt;strong&gt;RR&lt;/strong&gt;). I try very hard to do reproducible research. It is often hard. Often this is not clear how to achieve &lt;strong&gt;RR&lt;/strong&gt;. Within our group, we had recently some discussions about &lt;strong&gt;RR&lt;/strong&gt;. Below is a small &lt;em&gt;personal&lt;/em&gt; manifesto for &lt;strong&gt;RR&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;manifesto-for-reproducible-research&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Manifesto for reproducible research&lt;/h1&gt;
&lt;p&gt;I believe that good science is made of trustable science. I believe that the most trustable academic research can only be achieved with a rigorous application of scientific methods. I believe that a minimal condition of the scientific methods is reproducibility of the research. While being viewed as obvious consent within Academia, in practice it requires extremely well organised scientists. Statistics is by nature an interdisciplinary effort and as many other disciplines it faces the reproducibility crisis. If one want to produce relevant, widely accessible and trustable scientific outputs one has to take it very seriously.&lt;/p&gt;
&lt;p&gt;I view the reproducible research approach as a comprehensive philosophy. It includes the individual research of academic group members but also external collaborations. An essential part of reproducibility is the transparency of data. Therefore I tried to use publicly available and trustable data wherever possible and feasible. Likewise, I make my data product openly accessible together with the necessary documentation.&lt;/p&gt;
&lt;p&gt;I believe that scripting is the optimal way to achieve reproducible workflow. In order to create easily reproducible software packages, I follow the dynamic programming approach, which is a method to solve large scale problems by atomising them into simple tasks. Using version control allows me to document changes, ensuring historical reproducibility and efficient collaboration. I pay a special attention to publish the necessary documentation together with my softwares.&lt;/p&gt;
&lt;p&gt;Producing and delivering reproducible code implies being as independent as possible of the user environment. This is why I use platform independent and open source programming languages. It also requires to produce well documented code that corresponds to commonly used code styles, which facilitates user readability. I believe that the tests used to develop code are part of the code and then should be published.&lt;/p&gt;
&lt;p&gt;I believe that complex scientific challenges require large collaborative work to be tackled. I believe that reproducibility is of higher importance there. This is why I aim at working in an organised manner which means to be sparse with the documents I exchange to ensure efficient partnership. I believe that continuous integration is a way to save precious time.&lt;/p&gt;
&lt;p&gt;Reproducible research is a fast moving research area and I invest time for scooting new approaches and exchange with other research groups.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic SIR</title>
      <link>/post/gillespie/</link>
      <pubDate>Sun, 03 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/gillespie/</guid>
      <description>&lt;p&gt;I did my &lt;a href=&#34;http://www.math.uzh.ch/li/index.php?file&amp;amp;key1=41327&#34;&gt;Master Thesis&lt;/a&gt;, about &lt;strong&gt;Infectious Disease Inference&lt;/strong&gt;, at Stockholm University in the departement of &lt;a href=&#34;http://www.math.su.se/english/education/programmes/mathematical-statistics&#34;&gt;Mathematical Statistics&lt;/a&gt;. Infectious disease inference is a very interesting and challenging research subject. Moreover this domain of statistcs is at the edge between statistics and simulations (which made my background in physics very relevant and suddenly make my stats and physics master start resonating). I think that this was my very first contact with computational statistics and it gave me the desir to learn more!&lt;/p&gt;
&lt;p&gt;Here is a little bit of epidemic modelling. A detailed version (with references, codes and examples) of this introduction can be found &lt;a href=&#34;http://www.math.uzh.ch/li/index.php?file&amp;amp;key1=41327&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;sir-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SIR model&lt;/h1&gt;
&lt;p&gt;The mathematical modelling of infectious diseases (aka Infectious Disease Inference) is a tool to study the mechanisms by which diseases spread, to predict the future course of an outbreak and to evaluate strategies to control an epidemic. In the class of &lt;a href=&#34;https://en.wikipedia.org/wiki/Multi-compartment_model&#34;&gt;compartmental models&lt;/a&gt;, which serve as a base mathematical framework for understanding the complex dynamics of infectious diseases, the Susceptible-Infectious-Recovered (&lt;a href=&#34;https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model&#34;&gt;SIR&lt;/a&gt;) model is a valuable model for many infectious diseases.&lt;/p&gt;
&lt;div id=&#34;deterministic-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deterministic setting&lt;/h2&gt;
&lt;p&gt;When dealing with large populations deterministic models are often used. In deterministic compartmental models, the transition rates from one compartment to another are mathematically expressed as derivatives. Hence the model is formulated using ordinary differential equations (&lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_differential_equation&#34;&gt;ODE&lt;/a&gt;). A closed population is assumed at all times &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(P(t)=S(t)+I(t)+R(t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{dS(t)}{dt} = - \beta S(t)\frac{I(t)}{P(t)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{dI(t)}{dt} = \beta S(t)\frac{I(t)}{P(t)} - \gamma I(t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{dR(t)}{dt} = \gamma I(t)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are two example of deterministic modelling with and without birth and death process in the compartments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##package
library(deSolve)

##################################################
### Basic model SIR simulations
###################################################

##function
sir &amp;lt;- function(time, state, parameters) {
  with(as.list(c(state, parameters)), {
    dS &amp;lt;- -beta * S * I
    dI &amp;lt;- beta * S * I - gamma * I
    dR &amp;lt;- gamma * I
    
    return(list(c(dS, dI, dR)))
  })
}

##############################################################################
init &amp;lt;- c(S = 1-1e-6, I = 1e-6, 0.0)
parameters &amp;lt;- c(beta = 0.15, gamma = 0.005)
times &amp;lt;- seq(0, 1000, by = 1)
out &amp;lt;- as.data.frame(ode(y = init, times = times, func = sir, parms = parameters))
out$time &amp;lt;- NULL

par(cex=1.5)

matplot(times, out, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;Time&amp;quot;, ylab = &amp;quot;Population&amp;quot;, main = &amp;quot;SIR Model&amp;quot;, lwd = 1, lty = 1, bty = &amp;quot;l&amp;quot;, col = 2:4)
legend(x = 500,y = 0.6,c(&amp;quot;Susceptibles&amp;quot;, &amp;quot;Infecteds&amp;quot;, &amp;quot;Recovereds&amp;quot;), pch = 16, col = 2:4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gillespie_files/figure-html/cars-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##################################################
### Model SIR with birth and dead rate 
###################################################


sir_bd &amp;lt;- function(time, state, parameters) {
  with(as.list(c(state, parameters)), {
    dS &amp;lt;- birth - beta*S*I - death*S
    dI &amp;lt;- beta*S*I - gamma*I - death*I
    dR &amp;lt;- gamma*I - death*R
    
    return(list(c(dS, dI, dR)))
  })
}


##############################################################################
init &amp;lt;- c(S = 1-1e-6, I = 1e-6, R = 0.0)
parameters &amp;lt;- c(beta = 0.15, gamma = 0.005, birth =0.001, death=0.001)
times &amp;lt;- seq(0, 1000, by = 1)
out &amp;lt;- as.data.frame(ode(y = init, times = times, func = sir_bd, parms = parameters))
out$time &amp;lt;- NULL

matplot(times, out, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;Time&amp;quot;, ylab = &amp;quot;Population&amp;quot;, main = &amp;quot;SIR Model including birth and death rates&amp;quot;, lwd = 1, lty = 1, bty = &amp;quot;l&amp;quot;, col = 2:4)
legend(x = 500,y = 0.6,c(&amp;quot;Susceptibles&amp;quot;, &amp;quot;Infecteds&amp;quot;, &amp;quot;Recovereds&amp;quot;), pch = 16, col = 2:4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gillespie_files/figure-html/cars-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stochastic-modelling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stochastic modelling&lt;/h2&gt;
&lt;p&gt;A stochastic SIR model is defined analogously as the deterministic model. A closed homogeneous population is assumed, and &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(I(t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R(t)\)&lt;/span&gt; have the same definition as in the deterministic setting. As done previously, birth and death are ignored in this simple setting. The dynamic of the model is defined as follows. Infectives have contact with susceptibles at a constant rate &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Contacts are mutually independent. Any susceptible which is in contact with an infected individual immediately becomes infective and starts spreading the disease following the same rules. Infected individuals remain infectious for a random amount of time governed by an infections period distribution after which they stop being infectious and recover. The infectious periods are defined to be independent and identically distributed (also independent of the contact processes). The exponential distribution with intensity parameter &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; has received special attention in the literature, because the resulting model is Markovian. A stochastic process is said to be a Markov process or have the markov property if the conditional probability of the future state conditional on both present and past states depends only on the present state of the process.&lt;/p&gt;
&lt;p&gt;A simple and efficient algorithm to simulate a stochastic model is the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gillespie_algorithm&#34;&gt;Gillespie algorithm&lt;/a&gt;&lt;/em&gt;. This algorithm assumes that all possible transitions between compartments occur independently and are simulated at each time step with constant probability per unit time that depends on the current state of the system (i.e the number of individual in each compartment). The idea is to sample the next time event from an exponential distribution. Then, the event is randomly chosen amongst the possible transitions between compartments with probabilities proportional to their individual rates. The code below shows two realizations for a small and a large population of a stochastic SIR model simulated by a Gillespie algorithm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parms=c(m=0,b=0.02,v=0.1,r=0.3)

# define how state variables S, I and R change for each process
processes &amp;lt;- matrix(c(1,1,1,
                      -1,0,0,
                      0,-1,0,
                      0,0,-1,
                      -1,1,0,
                      0,-1,0,
                      0,-1,1), nrow=7, ncol=3, byrow = TRUE,
                    dimnames=list(c(&amp;quot;birth&amp;quot;,
                                    &amp;quot;death.S&amp;quot;,
                                    &amp;quot;death.I&amp;quot;,
                                    &amp;quot;death.R&amp;quot;,
                                    &amp;quot;infection&amp;quot;,
                                    &amp;quot;death.infec&amp;quot;,
                                    &amp;quot;recovery&amp;quot;),
                                  c(&amp;quot;dS&amp;quot;,&amp;quot;dI&amp;quot;,&amp;quot;dR&amp;quot;)))

##import necessary files/fct
source(file=&amp;quot;RFun/gillespie_fct_R.R&amp;quot;)

##small population
initial.state=c(S=97, I=3, R=0)

res&amp;lt;-gillespie(parms=parms, X0=initial.state, time.window=c(0,1000), processes=processes,pb = FALSE)
par(cex=1.5)
matplot(x = res[,1], y = res[,-1], type = &amp;quot;l&amp;quot;, lty = 1, xlab = &amp;quot;Time&amp;quot;, ylab = &amp;quot;Population&amp;quot;,col = 2:4, ylim=c(0,100), xlim=c(0,10), main=&amp;quot;One realization of a stochastic model \n for S(0)=97, I(0)=3, R(0)=0&amp;quot;)
legend(x=6,y=50,c(&amp;quot;Susceptibles&amp;quot;, &amp;quot;Infecteds&amp;quot;, &amp;quot;Recovereds&amp;quot;), pch = 16, col = 2:4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gillespie_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##huge population
initial.state=c(S=997, I=3, R=0)

res&amp;lt;-gillespie(parms=parms, X0=initial.state, time.window=c(0,10000), processes=processes,pb = FALSE)
matplot(x = res[,1], y = res[,-1], type = &amp;quot;l&amp;quot;, lty = 1, xlab = &amp;quot;Time&amp;quot;, ylab = &amp;quot;Population&amp;quot;,col = 2:4, ylim=c(0,1000), xlim=c(0,10), main=&amp;quot;One realization of a stochastic model \n for S(0)=997, I(0)=3, R(0)=0&amp;quot;)
legend(x=6,y=500,c(&amp;quot;Susceptibles&amp;quot;, &amp;quot;Infecteds&amp;quot;, &amp;quot;Recovereds&amp;quot;), pch = 16, col = 2:4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gillespie_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In a next post I will comment on the code of the parallel gillespie R function …&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Characterization of the Vitrocell® 24/48 aerosol exposure system for its use in exposures to liquid aerosols</title>
      <link>/publication/sandro1/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0200</pubDate>
      
      <guid>/publication/sandro1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A systems toxicology approach for comparative assessment: Biological impact of an aerosol from a candidate modified-risk tobacco product and cigarette smoke on human organotypic bronchial epithelial cultures.</title>
      <link>/publication/iskendar/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0100</pubDate>
      
      <guid>/publication/iskendar/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A new fluorescence-based method for characterizing in vitro aerosol exposure systems</title>
      <link>/publication/sandro2/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0100</pubDate>
      
      <guid>/publication/sandro2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Google Ngram</title>
      <link>/post/ngram/</link>
      <pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/ngram/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://books.google.com/ngrams&#34;&gt;Google Ngram Viewer&lt;/a&gt;, started in December 2010, is an online search engine that returns the yearly relative frequency of a set of words, found in a selected printed sources, called corpus of books, between 1500 and 2016 (many language available). More specifically, it returns the relative frequency of the yearly &lt;strong&gt;ngram&lt;/strong&gt; (continuous set of n words. For example, &lt;strong&gt;I&lt;/strong&gt; is a 1-gram and &lt;strong&gt;I am&lt;/strong&gt; is a 2-grams). This means that if you search for &lt;em&gt;one&lt;/em&gt; word (called unigram), you get the percentage of this word to all the other word found in the corpus of books for a certain year. When I discovered it, I was shocked! With &lt;strong&gt;google ngram&lt;/strong&gt; one can plot the yearly relative frequency of &lt;em&gt;any&lt;/em&gt; ngram! The &lt;a href=&#34;https://books.google.com/ngrams&#34;&gt;Google Ngram Viewer&lt;/a&gt; help page contains a lot of very convincing examples. It is the perfect typical data mining tool. I started to imagine what I could do with such a tool. One can study trends in knowledge, one can study relative importance of concepts etc …&lt;/p&gt;
&lt;p&gt;A &lt;a href=&#34;http://science.sciencemag.org/content/331/6014/176.full&#34;&gt;science paper&lt;/a&gt; has been published in 2011. They named this research field as &lt;em&gt;culturomics&lt;/em&gt;. This paper presents some convincing search examples, which put the &lt;em&gt;culturomics&lt;/em&gt; in front of the scientific literature. This research field has popping up in social science articles and made the Google ngrams charts very common and popular. This Google’ service have received many criticisms. A &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0137041&#34;&gt;PLOS paper&lt;/a&gt; discuss the pitfalls of Google ngrams. Mainly inaccurate character recognition, an overabundance of scientific literature, misindexation and misdating of texts. Up to that point, I though that this was manageable. Indeed, inaccuracies will be soon corrected and Google will incorporate other books then this will not be a problem. Last criticism is that the percentage take into account published manuscripts regardless of their importance. Again this paper discuss from in a very convincing way the limitations of Google ngram.&lt;/p&gt;
&lt;p&gt;We definitely need an R package to test it. Hopefully, it exists &lt;a href=&#34;https://github.com/seancarmody/ngramr&#34;&gt;ngramr&lt;/a&gt;, an r package available on github. It is very efficient and easy to use. Then let us start with our first search (an inspired joke by &lt;a href=&#34;http://xkcd.com/1007/&#34;&gt;xkcd&lt;/a&gt; actually).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(devtools)
install_github(&amp;quot;ngramr&amp;quot;, &amp;quot;seancarmody&amp;quot;)
library(ngramr)

data&amp;lt;-ngrami(c(&amp;quot;sustainability&amp;quot;))

library(ggplot2)
data.sub&amp;lt;-subset(data,data$Year&amp;gt;1985)

p&amp;lt;- ggplot(data,aes(x = Year, y=Frequency))+
  geom_point()+
  stat_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE,data=data, fullrange = TRUE)+
  scale_y_log10(limits=c(0.00000000001,1))+
  xlim(1945,2100)+
  annotate(&amp;quot;pointrange&amp;quot;, x = 2072, y = 0.5, ymin = 0.01, ymax = 0.8,
           colour = &amp;quot;red&amp;quot;, size = 1.5)+
  annotate(&amp;quot;text&amp;quot;, x = 2072, y = 0.001, label = &amp;quot;100% of words are Sustainability&amp;quot;,colour=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which produces this graph. Fast and easy! As one can see, on a log scale Google ngram predicts that in a near future all published literature will be made of the word “Sustainability” written over and over …&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/ngram_files/figure-html/pressure-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>abn use accross the world?</title>
      <link>/post/abn_used/</link>
      <pubDate>Sat, 03 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/abn_used/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;p&gt;This post is highly inspired by &lt;a href=&#34;https://www.r-bloggers.com/my-r-packages-worldmap-of-downloads/&#34;&gt;this post&lt;/a&gt; from R-Bloggers and my &lt;a href=&#34;#IDdownload&#34;&gt;previous post&lt;/a&gt;. Again the necessary code is available &lt;a href=&#34;&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As maintenair of an R package, it is always interesting to know if it is used and by whom? This post shows some plots to asses R package usage.&lt;/p&gt;
&lt;p&gt;I started by a small review to list the similar other R packages. I found that &lt;a href=&#34;https://cran.r-project.org/web/packages/bnlearn/index.html&#34;&gt;bnlearn&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/deal/index.html&#34;&gt;deal&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/gRbase/index.html&#34;&gt;grbase&lt;/a&gt; are comparable to &lt;a href=&#34;https://cran.r-project.org/web/packages/abn/index.html&#34;&gt;abn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here is a comparative scatter plot of the daily count of downloads, from May the 1st to September the 4th, for the listed package.&lt;/p&gt;
&lt;div style=&#34;width:800px; height=600px&#34;&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/images/dowloads_comp.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Here is the ordered total number of download per country for the same period for abn only.&lt;/p&gt;
&lt;div style=&#34;width:800px; height=600px&#34;&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/images/country_dowloads_abn.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Here is the total count over the week days for this period. As one can see, number of downloads decreases of about (only?) a half during the weekend.&lt;/p&gt;
&lt;div style=&#34;width:800px; height=600px&#34;&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/images/downloads_weekdays_abn.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This graph represents the instantly (smoothed by minute) number of downloads in standardized UTC.&lt;/p&gt;
&lt;div style=&#34;width:800px; height=600px&#34;&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/images/downloads_hour_abn.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Finaly a world map with number of downloads on log scale.&lt;/p&gt;
&lt;div style=&#34;width:800px; height=600px&#34;&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/images/downloads_abn_mapW.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Below is the necessary code to extract, manipulate and plot the data.&lt;/p&gt;
&lt;p&gt;  &lt;a id=&#34;my_chunk&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;###########################################################################
##Gilles Kratzer
##Analysis on CRAN Rstudio log files in order to compute to the number of downloads
##History :
## --2016/11/13 document created
###########################################################################

###########################################################################
##Purpose: Analysis of the downloads of R packages
##Highly inspired by https://www.r-bloggers.com/finally-tracking-cran-packages-downloads/
##Highly inspired by http://blog.kongscn.me/2015/05/15/r-packages-stats.html
##Highly inspired by https://www.r-bloggers.com/my-r-packages-worldmap-of-downloads/
###########################################################################

##cleaning
rm(list = ls())
graphics.off()

##packages
library(lubridate)
library(data.table)
library(dplyr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(maptools)



## ======================================================================
## Plot world map
## ======================================================================


  counts &amp;lt;- cbind.data.frame(table(data$country))
  names(counts) &amp;lt;- c(&amp;quot;country&amp;quot;, &amp;quot;count&amp;quot;)
  
  # you need to download a shapefile of the world map from Natural Earth (for instance)
  # http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip
  # and unzip it in the &amp;#39;shp.file.repos&amp;#39; repository
  world&amp;lt;-readShapePoly(fn=paste(shp.file.repos, &amp;quot;ne_110m_admin_0_countries&amp;quot;, sep=&amp;quot;/&amp;quot;))
  ISO_full &amp;lt;- as.character(world@data$iso_a2)
  ISO_full[146] &amp;lt;- &amp;quot;SOM&amp;quot;  # The iso identifier for the Republic of Somaliland is missing
  ISO_full[89]  &amp;lt;- &amp;quot;KV&amp;quot; # as for the Republic of Kosovo
  ISO_full[39]  &amp;lt;- &amp;quot;CYP&amp;quot; # as for Cyprus
  
  colcode &amp;lt;- numeric(length(ISO_full))
  names(colcode) &amp;lt;- ISO_full
  dnl_places &amp;lt;- names(colcode[which(names(colcode) %in% as.character(counts$country))])
  rownames(counts) &amp;lt;- counts$country
  colcode[dnl_places] &amp;lt;- counts[dnl_places, &amp;quot;count&amp;quot;]
  
  world@data$id &amp;lt;- rownames(world@data)
  world.points &amp;lt;- fortify(world, by=&amp;quot;id&amp;quot;)
  names(colcode) &amp;lt;- rownames(world@data)
  world.points$dnls &amp;lt;- colcode[world.points$id]
  
  world.map &amp;lt;-  ggplot(data=world.points) +
    geom_polygon(aes(x = long, y = lat, group=group, fill=dnls), color=&amp;quot;black&amp;quot;) +
    coord_equal() + #theme_minimal() +
    scale_fill_gradient(low=&amp;quot;white&amp;quot;, high=&amp;quot;#56B1F7&amp;quot;, name=&amp;quot;Downloads&amp;quot;) +
    labs(title=paste(pkgname, &amp;quot; downloads from the &amp;#39;0-Cloud&amp;#39; CRAN mirror by country\nfrom &amp;quot;, date.start, &amp;quot; to &amp;quot;, date.stop,&amp;quot;\n(Total downloads: &amp;quot;, sum(counts$count), &amp;quot;)&amp;quot;, sep=&amp;quot;&amp;quot;))
world.map&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
